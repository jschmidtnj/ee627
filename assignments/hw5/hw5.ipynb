{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5\n",
    "\n",
    "> I pledge my honor that I have abided by the Stevens Honor System - Joshua Schmidt 3/27/21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nowadays, technology companies often use recommender algorithms to recommend products, music, movie, etc. The recommender algorithm is really useful and helps all those companies make a huge profit.\n",
    "- Now, after learning the factorial matrix and other useful algorithms. It's your turn to solve the problem which Spotify, Amazon, and Yahoo Music faced every day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*- Overview *\n",
    "\n",
    "trainIterm2.txt - the training set\n",
    "\n",
    "testIterm2.txt - the test set\n",
    "\n",
    "sample_ submission.csv - a sample submission file in the correct format\n",
    "\n",
    "trackData2.txt -- Track information formatted as:\n",
    "\n",
    "<'TrackId'>|<'AlbumId'>|<'ArtistId'>|<'Optional GenreId_1'>|...|<'Optional GenreId_k'>\n",
    "\n",
    "albumData2.txt -- Album information formatted as:\n",
    "\n",
    "<'AlbumId'>|<'ArtistId'>|<'Optional GenreId_1'>|...|<'Optional GenreId_k'>\n",
    "\n",
    "artistData2.txt -- Artist listing formatted as:\n",
    "\n",
    "<'ArtistId'>\n",
    "\n",
    "genreData2.txt -- Genre listing formatted as:\n",
    "\n",
    "<'GenreId'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional, Set\n",
    "from collections import OrderedDict\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_seperator = '|'\n",
    "\n",
    "def load_training_dataset(file_path: str) -> csr_matrix:\n",
    "    \"\"\"\n",
    "    returns a sparse matrix with columns of user ids and rows of track ids\n",
    "    cell values are the rating for the given track\n",
    "    \"\"\"\n",
    "    all_tracks: Set[int] = set()\n",
    "    all_users: Set[int] = set()\n",
    "    with open(file_path) as file_data:\n",
    "        lines_remaining: int = 0\n",
    "        current_user: Optional[int] = None\n",
    "        for line in file_data:\n",
    "            line = line.strip()\n",
    "            if lines_remaining == 0:\n",
    "                if user_seperator not in line:\n",
    "                    raise ValueError('cannot find user seperator')\n",
    "                current_user, lines_remaining = map(int, line.split(user_seperator))\n",
    "                all_users.add(current_user)\n",
    "            else:\n",
    "                track_id, _score = map(int, line.split())\n",
    "                all_tracks.add(track_id)\n",
    "                lines_remaining -= 1\n",
    "\n",
    "        all_track_indexes = {k: v for v, k in enumerate(sorted(list(all_tracks)))}\n",
    "\n",
    "        output = csr_matrix((len(all_tracks), len(all_users)), dtype=np.uint8)\n",
    "\n",
    "        lines_remaining = 0\n",
    "        user_index: int = -1\n",
    "        for line in file_data:\n",
    "            line = line.strip()\n",
    "            if lines_remaining == 0:\n",
    "                if user_seperator not in line:\n",
    "                    raise ValueError('cannot find user seperator')\n",
    "                _current_user, lines_remaining = map(int, line.split(user_seperator))\n",
    "                user_index += 1\n",
    "            else:\n",
    "                track_id, score = map(int, line.split())\n",
    "                track_index = all_track_indexes[track_id]\n",
    "                output[track_index][user_index] = score\n",
    "                lines_remaining -= 1\n",
    "\n",
    "        return output\n",
    "\n",
    "training_data = load_training_dataset('data/trainItem2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOL=1e-10\n",
    "MAX_ITERS = 3\n",
    "\n",
    "def converged(Z, d_norm):\n",
    "    err = np.linalg.norm(Z, 'fro') / d_norm\n",
    "    print('error: ', err)\n",
    "    return err < TOL\n",
    "\n",
    "def shrink(M, tau):\n",
    "    S = np.abs(M) - tau\n",
    "    return np.sign(M) * np.where(S>0, S, 0)\n",
    "\n",
    "def _svd(M, rank): \n",
    "    return fbpca.pca(M, k=min(rank, np.min(M.shape)), raw=True)\n",
    "\n",
    "def norm_op(M): \n",
    "    return _svd(M, 1)[1][0]\n",
    "\n",
    "def svd_reconstruct(M, rank, min_sv):\n",
    "    u, s, v = _svd(M, rank)\n",
    "    s -= min_sv\n",
    "    nnz = (s > 0).sum()\n",
    "    return u[:,:nnz] @ np.diag(s[:nnz]) @ v[:nnz], nnz\n",
    "\n",
    "def pcp(X, maxiter=10, k=10):\n",
    "    m, n = X.shape\n",
    "    trans = m < n\n",
    "\n",
    "    if trans: \n",
    "        X = X.T; m, n = X.shape\n",
    "\n",
    "    lamda = 1/np.sqrt(m)\n",
    "\n",
    "    op_norm = norm_op(X)\n",
    "\n",
    "    Y = np.copy(X) / max(op_norm, np.linalg.norm( X, np.inf) / lamda)\n",
    "\n",
    "    mu = k*1.25/op_norm; mu_bar = mu * 1e7; rho = k * 1.5\n",
    "\n",
    "    d_norm = np.linalg.norm(X, 'fro')\n",
    "    L = np.zeros_like(X); sv = 1\n",
    "    for i in range(maxiter):\n",
    "        print(\"rank sv:\", sv)\n",
    "        X2 = X + Y/mu\n",
    "\n",
    "        # update estimate of Sparse Matrix by \"shrinking/truncating\": original - low-rank\n",
    "        S = shrink(X2 - L, lamda/mu)\n",
    "\n",
    "        # update estimate of Low-rank Matrix by doing truncated SVD of rank sv & reconstructing.\n",
    "        # count of singular values > 1/mu is returned as svp\n",
    "        L, svp = svd_reconstruct(X2 - S, sv, 1/mu)\n",
    "\n",
    "        # If svp < sv, you are already calculating enough singular values.\n",
    "        # If not, add 20% (in this case 240) to sv\n",
    "        sv = svp + (1 if svp < sv else round(0.05*n))\n",
    "\n",
    "        # residual\n",
    "        Z = X - L - S\n",
    "        Y += mu * Z\n",
    "        mu *= rho\n",
    "\n",
    "        if m > mu_bar:\n",
    "            m = mu_bar\n",
    "\n",
    "        if converged(Z, d_norm): \n",
    "            break\n",
    "\n",
    "    if trans:\n",
    "        L=L.T; S=S.T\n",
    "\n",
    "    return L, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L, S = pcp(training_data)\n",
    "import delayedsparse\n",
    "pca=delayedsparse.PCA(n_components=5)\n",
    "pca.fit(training_data)\n",
    "Xmaped=pca.transform(training_data)\n",
    "print(Xmaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 108. GiB for an array with shape (295799, 49204) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-da23b2907974>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparsePCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtransformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparsePCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# rpca = R_pca(training_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ee627/lib/python3.7/site-packages/sklearn/decomposition/_sparse_pca.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 108. GiB for an array with shape (295799, 49204) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import SparsePCA\n",
    "transformer = SparsePCA(n_components=5, random_state=0)\n",
    "transformer.fit(training_data.toarray())\n",
    "# rpca = R_pca(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num users: 20000\n"
     ]
    }
   ],
   "source": [
    "user_seperator = '|'\n",
    "\n",
    "user_data: Dict[int, List[int]] = OrderedDict()\n",
    "current_user: Optional[int] = None\n",
    "lines_remaining = 0\n",
    "\n",
    "with open('data/testItem2.txt') as test_data:\n",
    "    for line in test_data:\n",
    "        if lines_remaining == 0:\n",
    "            if user_seperator not in line:\n",
    "                raise ValueError('cannot find user seperator')\n",
    "            current_user, lines_remaining = map(int, line.split(user_seperator))\n",
    "        else:\n",
    "            if current_user not in user_data:\n",
    "                user_data[current_user] = np.array([], dtype=np.int)\n",
    "            user_data[current_user] = np.append(user_data[current_user], int(line.strip()))\n",
    "            lines_remaining -= 1\n",
    "\n",
    "print(f'num users: {len(user_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num selections per user: (3,)\n"
     ]
    }
   ],
   "source": [
    "num_pick = 3\n",
    "\n",
    "selections: Dict[int, List[int]] = OrderedDict()\n",
    "\n",
    "for user_id, options in user_data.items():\n",
    "    indexes = np.random.choice(options.shape[0], size=num_pick, replace=False)\n",
    "    selections[user_id] = options[indexes]\n",
    "\n",
    "print(f'num selections per user: {selections[user_id].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_track_ids: List[str] = []\n",
    "predictors: List[int] = []\n",
    "\n",
    "for user_id, tracks in user_data.items():\n",
    "    for track_id in tracks:\n",
    "        user_track_ids.append(f'{user_id}_{track_id}')\n",
    "        predictors.append(1 if track_id in selections[user_id] else 0)\n",
    "\n",
    "output_df = pd.DataFrame({'TrackID': user_track_ids, 'Predictor': predictors}).set_index('TrackID')\n",
    "output_df.to_csv('data/output.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee627",
   "language": "python",
   "name": "ee627"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
