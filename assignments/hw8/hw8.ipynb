{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 8\n",
    "\n",
    "> I pledge my Honor that I have abided by the Stevens Honor System. - Joshua Schmidt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 0\n",
    "\n",
    "spark 1 example code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"hw8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "data = sc.textFile('re_u.data')\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['196,242,3', '186,302,3', '22,377,1', '244,51,2', '166,346,1']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = data.map(lambda line: line.split(',')).map(lambda line: Rating(int(line[0]), int(line[1]), float(line[2])))\n",
    "ratings.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=196, product=242, rating=3.0),\n",
       " Rating(user=186, product=302, rating=3.0),\n",
       " Rating(user=22, product=377, rating=1.0),\n",
       " Rating(user=244, product=51, rating=2.0),\n",
       " Rating(user=166, product=346, rating=1.0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternating least squares\n",
    "rank = 10\n",
    "num_iter = 10\n",
    "model = ALS.train(ratings, rank, num_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error: 0.483\n"
     ]
    }
   ],
   "source": [
    "test_data = ratings.map(lambda elem: (elem[0], elem[1]))\n",
    "predictions = model.predictAll(test_data).map(lambda pred: ((pred[0], pred[1]), pred[2]))\n",
    "ratings_predictions = ratings.map(lambda rating: ((rating[0], rating[1]), rating[2])).join(predictions)\n",
    "mse = ratings_predictions.map(lambda elem: (elem[1][0] - elem[1][1]) ** 2).mean()\n",
    "print(f'mean squared error: {mse:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "final project recommendation code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"hw8\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+\n",
      "|   _c0|   _c1|_c2|\n",
      "+------+------+---+\n",
      "|199808|248969| 90|\n",
      "|199808|  2663| 90|\n",
      "|199808| 28341| 90|\n",
      "|199808| 42563| 90|\n",
      "|199808| 59092| 90|\n",
      "+------+------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training = spark.read.csv(\"trainItem.data\", header = False)\n",
    "training.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+\n",
      "|userID|itemID|rating|\n",
      "+------+------+------+\n",
      "|199808|248969|    90|\n",
      "|199808|  2663|    90|\n",
      "|199808| 28341|    90|\n",
      "|199808| 42563|    90|\n",
      "|199808| 59092|    90|\n",
      "+------+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training = training.withColumnRenamed(\"_c0\", \"userID\").withColumnRenamed(\"_c1\", \"itemID\").withColumnRenamed(\"_c2\", \"rating\")\n",
    "training.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+\n",
      "|userID|itemID|rating|\n",
      "+------+------+------+\n",
      "|199808|248969|  90.0|\n",
      "|199808|  2663|  90.0|\n",
      "|199808| 28341|  90.0|\n",
      "+------+------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "training = training.withColumn(\"userID\", training[\"userID\"].cast(IntegerType()))\n",
    "training = training.withColumn(\"itemID\", training[\"itemID\"].cast(IntegerType()))\n",
    "training = training.withColumn(\"rating\", training[\"rating\"].cast('float'))\n",
    "training.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ALS model\n",
    "als = ALS(\n",
    "    maxIter=5,\n",
    "    rank = 5,\n",
    "    regParam=0.01,\n",
    "    userCol=\"userID\",\n",
    "    itemCol=\"itemID\",\n",
    "    ratingCol=\"rating\",\n",
    "    nonnegative = True,\n",
    "    implicitPrefs = False,\n",
    "    coldStartStrategy=\"drop\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = spark.read.csv(\"testItem.data\", header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+\n",
      "|userID|itemID|rating|\n",
      "+------+------+------+\n",
      "|199810|208019|     0|\n",
      "|199810| 74139|     0|\n",
      "|199810|  9903|     0|\n",
      "|199810|242681|     0|\n",
      "|199810| 18515|     0|\n",
      "+------+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testing = testing.withColumnRenamed(\"_c0\", \"userID\").withColumnRenamed(\"_c1\", \"itemID\").withColumnRenamed(\"_c2\", \"rating\")\n",
    "testing.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+\n",
      "|userID|itemID|rating|\n",
      "+------+------+------+\n",
      "|199810|208019|   0.0|\n",
      "|199810| 74139|   0.0|\n",
      "|199810|  9903|   0.0|\n",
      "+------+------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testing = testing.withColumn(\"userID\", testing[\"userID\"].cast(IntegerType()))\n",
    "testing = testing.withColumn(\"itemID\", testing[\"itemID\"].cast(IntegerType()))\n",
    "testing = testing.withColumn(\"rating\", testing[\"rating\"].cast('float'))\n",
    "testing.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+----------+\n",
      "|userID|itemID|rating|prediction|\n",
      "+------+------+------+----------+\n",
      "|230073|   463|   0.0|113.081635|\n",
      "|230962|   471|   0.0| 84.517975|\n",
      "|218845|  1088|   0.0| 53.643383|\n",
      "|209697|  1088|   0.0| 21.104813|\n",
      "|224445|  2142|   0.0| 33.803844|\n",
      "+------+------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(testing)\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe into a folder 'predictions' \n",
    "# with a single file (coalesce(1))\n",
    "# but you can not assign the filename\n",
    "predictions.coalesce(1).write.csv(\"predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userID  itemID  rating  prediction\n",
      "0  230073     463     0.0  113.081635\n",
      "1  230962     471     0.0   84.517975\n",
      "2  218845    1088     0.0   53.643383\n",
      "3  209697    1088     0.0   21.104813\n",
      "4  224445    2142     0.0   33.803844\n"
     ]
    }
   ],
   "source": [
    "# save dataframe to a single csv file \n",
    "df = predictions.toPandas()\n",
    "print(df.head())\n",
    "df.to_csv('myprediction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.recommendation import ALS\n",
    "# load data\n",
    "data = sc.textFile('re_u.data')\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['196,242,3', '186,302,3', '22,377,1', '244,51,2', '166,346,1']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = data.map(lambda line: line.split(',')).map(lambda line: Rating(int(line[0]), int(line[1]), float(line[2])))\n",
    "ratings.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=196, product=242, rating=3.0),\n",
       " Rating(user=186, product=302, rating=3.0),\n",
       " Rating(user=22, product=377, rating=1.0),\n",
       " Rating(user=244, product=51, rating=2.0),\n",
       " Rating(user=166, product=346, rating=1.0)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error for rank 5: 0.615\n",
      "mean squared error for rank 7: 0.547\n",
      "mean squared error for rank 10: 0.474\n",
      "mean squared error for rank 20: 0.293\n"
     ]
    }
   ],
   "source": [
    "# different ranks\n",
    "for rank in [5, 7, 10, 20]:\n",
    "    num_iter = 20\n",
    "    curr_model = ALS.train(ratings, rank, num_iter)\n",
    "    test_data = ratings.map(lambda elem: (elem[0], elem[1]))\n",
    "    predictions = curr_model.predictAll(test_data).map(lambda pred: ((pred[0], pred[1]), pred[2]))\n",
    "    ratings_predictions = ratings.map(lambda rating: ((rating[0], rating[1]), rating[2])).join(predictions)\n",
    "    mse = ratings_predictions.map(lambda elem: (elem[1][0] - elem[1][1]) ** 2).mean()\n",
    "    print(f'mean squared error for rank {rank}: {mse:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error for num_iter 2: 0.481\n",
      "mean squared error for num_iter 5: 0.337\n",
      "mean squared error for num_iter 10: 0.308\n",
      "mean squared error for num_iter 20: 0.291\n"
     ]
    }
   ],
   "source": [
    "# different num iterations\n",
    "for num_iter in [2, 5,10, 20]:\n",
    "    rank = 20\n",
    "    curr_model = ALS.train(ratings, rank, num_iter)\n",
    "    test_data = ratings.map(lambda elem: (elem[0], elem[1]))\n",
    "    predictions = curr_model.predictAll(test_data).map(lambda pred: ((pred[0], pred[1]), pred[2]))\n",
    "    ratings_predictions = ratings.map(lambda rating: ((rating[0], rating[1]), rating[2])).join(predictions)\n",
    "    mse = ratings_predictions.map(lambda elem: (elem[1][0] - elem[1][1]) ** 2).mean()\n",
    "    print(f'mean squared error for num_iter {num_iter}: {mse:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error for num_iter 20: 0.000\n",
      "mean squared error for num_iter 20: 0.001\n",
      "mean squared error for num_iter 20: 0.003\n",
      "mean squared error for num_iter 20: 0.026\n",
      "mean squared error for num_iter 20: 0.152\n",
      "mean squared error for num_iter 20: 0.292\n"
     ]
    }
   ],
   "source": [
    "# different num iterations\n",
    "import pyspark\n",
    "for data_size in [2000, 5000, 10000, 20000, 50000, 100000]:\n",
    "    rank = 20\n",
    "    num_iter = 20\n",
    "    curr_ratings = sc.parallelize(ratings.take(data_size))\n",
    "    curr_model = ALS.train(curr_ratings, rank, num_iter)\n",
    "    test_data = curr_ratings.map(lambda elem: (elem[0], elem[1]))\n",
    "    predictions = curr_model.predictAll(test_data).map(lambda pred: ((pred[0], pred[1]), pred[2]))\n",
    "    ratings_predictions = curr_ratings.map(lambda rating: ((rating[0], rating[1]), rating[2])).join(predictions)\n",
    "    mse = ratings_predictions.map(lambda elem: (elem[1][0] - elem[1][1]) ** 2).mean()\n",
    "    print(f'mean squared error for num_iter {num_iter}: {mse:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the above 3 different scenarios, from your observation, how MSE is changed related to the parameters? That is, which factor, the rank size, or the numIterations size, or the data size will change the MSE value more significantly?\n",
    "\n",
    "As rank increases from 5 to 20, mean squared error decreases. As the number of iterations increases, all else being equal, the mean squared error decreases. As the amount of data used for training and testing increases, the mean squared error increases. Data size will change the MSE value most significantly, because as the amount of training and testing data increases, there will be more outliers that will contribute to a higher MSE. The rank size seems to have the second highest effect on the MSE, with a range of 0.615 to 0.293 vs 0.481 to 0.291 for number of iterations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee627",
   "language": "python",
   "name": "ee627"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
